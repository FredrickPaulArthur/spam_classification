{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8350,"status":"ok","timestamp":1710951923796,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"quFjYeKLGftc","outputId":"3825f7e8-55f6-4f64-d927-a02f4ea108d5"},"outputs":[{"data":{"text/plain":["0       Subject: enron methanol ; meter # : 988291\\r\\n...\n","1       Subject: hpl nom for january 9 , 2001\\r\\n( see...\n","2       Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n","3       Subject: photoshop , windows , office . cheap ...\n","4       Subject: re : indian springs\\r\\nthis deal is t...\n","                              ...                        \n","5166    Subject: put the 10 on the ft\\r\\nthe transport...\n","5167    Subject: 3 / 4 / 2000 and following noms\\r\\nhp...\n","5168    Subject: calpine daily gas nomination\\r\\n>\\r\\n...\n","5169    Subject: industrial worksheets for august 2000...\n","5170    Subject: important online banking alert\\r\\ndea...\n","Name: text, Length: 5171, dtype: object"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer   # Term Frequency - Inverse Document Frequency\n","import spacy\n","import tensorflow as tf\n","nlp = spacy.load('en_core_web_sm')\n","\n","data = pd.read_csv(\n","    \"/content/drive/MyDrive/Datasets/spam_ham_dataset.csv\",\n","    #on_bad_lines=False,\n","    engine=\"python\"     # ParserError: Error tokenizing data. C error: EOF inside string starting at row 1989\n",")\n","\n","df = pd.DataFrame(data)\n","df.dropna()\n","\n","X = df[\"text\"]\n","y = df[\"label_num\"]\n","\n","X"]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1710951923797,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"PrXY4fCBwsJr","outputId":"c3fe1255-9e76-4689-d7a5-ac0ab396cf83"},"outputs":[{"data":{"text/plain":["0       subject: enron methanol ; meter # : 988291\\r\\n...\n","1       subject: hpl nom for january 9 , 2001\\r\\n( see...\n","2       subject: neon retreat\\r\\nho ho ho , we ' re ar...\n","3       subject: photoshop , windows , office . cheap ...\n","4       subject: re : indian springs\\r\\nthis deal is t...\n","                              ...                        \n","5166    subject: put the 10 on the ft\\r\\nthe transport...\n","5167    subject: 3 / 4 / 2000 and following noms\\r\\nhp...\n","5168    subject: calpine daily gas nomination\\r\\n>\\r\\n...\n","5169    subject: industrial worksheets for august 2000...\n","5170    subject: important online banking alert\\r\\ndea...\n","Name: text, Length: 5171, dtype: object"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# 1. Lower\n","X = X.str.lower()\n","\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1710951923797,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"Ma2t6DoZw7-g","outputId":"465d9afd-95dd-463e-aea5-7f20a55e3c98"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'subject  enron methanol   meter     988291  this is a follow up to the note i gave you on monday   4   3   00   preliminary  flow data provided by daren      please override pop   s daily volume   presently zero   to reflect daily  activity you can obtain from gas control    this change is needed asap for economics purposes  '"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 2. Remove special characters\n","spl_chars_removed = []\n","for sentence in X:\n","  spl_chars_removed.append(re.sub('[^A-Za-z0-9+]', \" \", sentence))\n","X = spl_chars_removed\n","\n","X[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":260395,"status":"ok","timestamp":1710952184173,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"j0oDN3Uu--mP","outputId":"3c16e373-aa03-43c8-a39c-c36f09539e8f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n  Lemmatization not required for email classification because the Model is going to predict on  certian keywords\\n  The test data is going to be lemmatized too, so lemmatization may help. Its required.\\n'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# 4. Lemmatize with Remove stopwords\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","tokenized_emails = []\n","for text in X:\n","  doc = nlp(text)   # for each email\n","  tokens = [token.lemma_ for token in doc if not (token.is_stop or token.is_space)]\n","\n","  tokenized_emails.append(\" \".join(tokens))\n","\n","X = tokenized_emails\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":857,"status":"ok","timestamp":1710954731123,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"wOvxKpiqyb2N","outputId":"ce554319-c0ae-4531-ac9a-8774f1a08f25"},"outputs":[{"data":{"text/plain":["['I', 'am', 'fred', 'let\"s']"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["nlp2 = spacy.blank('en')\n","doc2 = nlp2('I am fred eat\\'s')\n","\n","vocab = [token.text for token in nlp('I am fred let\"s')]\n","vocab"]},{"cell_type":"markdown","metadata":{"id":"jFLt3LgUeVIl"},"source":["TF-IDF vectorizer takes the vocabulary into account and calculates the Word Vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5494,"status":"ok","timestamp":1710952189662,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"BcI1L729UpJT","outputId":"86c47832-8f49-4eb3-9ac5-d68f50c924e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5171, 45991)\n","(4653, 45991)\n","[0 0 0 ... 0 0 1]\n"]}],"source":["# 5. Vectorize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","X_tfidf = vectorizer.fit_transform(X)    # vectorizer.transform(x) can be used for already fitted Vectorizer\n","\n","\n","X_tfidf_array = X_tfidf.toarray()\n","y = np.array(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_tfidf_array, y, test_size=0.1, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1710952189662,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"ZD-P71jhW9u8","outputId":"3e4cc17d-6ba6-4c58-f112-c6ada601a3c5"},"outputs":[{"data":{"text/plain":["((4653, 45991), (4653,))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, y_train.shape         # (5171, 46161)  - where 46161 is the size of the vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51525,"status":"ok","timestamp":1710952241171,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"KtoS_JqEBGyI","outputId":"a266a983-1a8d-43f4-b783-a63f50e0012d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","146/146 [==============================] - 4s 23ms/step - loss: 0.3766 - accuracy: 0.8367\n","Epoch 2/10\n","146/146 [==============================] - 4s 28ms/step - loss: 0.1106 - accuracy: 0.9918\n","Epoch 3/10\n","146/146 [==============================] - 6s 42ms/step - loss: 0.0459 - accuracy: 0.9970\n","Epoch 4/10\n","146/146 [==============================] - 5s 33ms/step - loss: 0.0243 - accuracy: 0.9989\n","Epoch 5/10\n","146/146 [==============================] - 5s 37ms/step - loss: 0.0148 - accuracy: 0.9996\n","Epoch 6/10\n","146/146 [==============================] - 6s 41ms/step - loss: 0.0098 - accuracy: 0.9998\n","Epoch 7/10\n","146/146 [==============================] - 3s 22ms/step - loss: 0.0070 - accuracy: 0.9998\n","Epoch 8/10\n","146/146 [==============================] - 3s 22ms/step - loss: 0.0051 - accuracy: 0.9998\n","Epoch 9/10\n","146/146 [==============================] - 4s 30ms/step - loss: 0.0039 - accuracy: 0.9998\n","Epoch 10/10\n","146/146 [==============================] - 5s 34ms/step - loss: 0.0030 - accuracy: 1.0000\n"]}],"source":["import tensorflow as tf\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(45991,)),\n","    tf.keras.layers.Dense(32),\n","    tf.keras.layers.Dense(1, activation='sigmoid'),\n","])\n","\n","\n","model.compile(\n","    loss=tf.keras.losses.BinaryCrossentropy(),\n","    optimizer=tf.keras.optimizers.Adam(),\n","    metrics=['accuracy']\n",")\n","\n","\n","history = model.fit(X_train, y_train, epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1141,"status":"ok","timestamp":1710955070909,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"atIq867PvY2l","outputId":"437a729b-5047-49d0-bbb8-99680ccdc319"},"outputs":[{"name":"stdout","output_type":"stream","text":["17/17 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9865\n"]}],"source":["test_loss, test_accuracy = model.evaluate(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"2mZt1D42xiMA"},"source":["### Model prediction function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388,"status":"ok","timestamp":1710955117269,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"0ehFhL5JuUIs","outputId":"194dba81-f9d4-46b8-e5e6-41ee91604a9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 25ms/step\n"]},{"data":{"text/plain":["['ham', 'spam', 'spam', 'ham', 'ham']"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["def model_predictions(model, X_predict):\n","  y_preds = model.predict(X_predict).reshape((X_predict.shape[0], ))  # the rows - number of emails\n","  preds_spam_ham = ['spam' if prob==1 else 'ham' for prob in np.round(y_preds)]\n","\n","  return preds_spam_ham\n","\n","\n","test_emails = vectorizer.transform(\n","  ['Dinner tonight in my house', 'free money lottery coupons now', 'youve won the lottery click the link', 'Breakfast today in John\\'s house', 'The principal asked us to leave']\n",").toarray()\n","\n","model_predictions(model, test_emails)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1175,"status":"ok","timestamp":1710953453684,"user":{"displayName":"Fredrick Paul A","userId":"15424905174411628688"},"user_tz":-330},"id":"MqaJpju-vY_s","outputId":"41aeb906-1692-4ffc-91ba-9e835bbdd5d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["17/17 [==============================] - 0s 11ms/step\n","['ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham']\n"]}],"source":["predictions = model_predictions(model, X_test)\n","print(predictions)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPxnKiw+7aFFxa87FyuycfM","mount_file_id":"1g4mdcU9TD0VyH630Ofo-5qVTYJB80ade","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
