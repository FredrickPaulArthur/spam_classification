{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8350,
     "status": "ok",
     "timestamp": 1710951923796,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "quFjYeKLGftc",
    "outputId": "3825f7e8-55f6-4f64-d927-a02f4ea108d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Subject: enron methanol ; meter # : 988291\\r\\n...\n",
       "1       Subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
       "2       Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
       "3       Subject: photoshop , windows , office . cheap ...\n",
       "4       Subject: re : indian springs\\r\\nthis deal is t...\n",
       "                              ...                        \n",
       "5166    Subject: put the 10 on the ft\\r\\nthe transport...\n",
       "5167    Subject: 3 / 4 / 2000 and following noms\\r\\nhp...\n",
       "5168    Subject: calpine daily gas nomination\\r\\n>\\r\\n...\n",
       "5169    Subject: industrial worksheets for august 2000...\n",
       "5170    Subject: important online banking alert\\r\\ndea...\n",
       "Name: text, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   # Term Frequency - Inverse Document Frequency\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"spam_ham_dataset.csv\",\n",
    "    #on_bad_lines=False,\n",
    "    engine=\"python\"     # ParserError: Error tokenizing data. C error: EOF inside string starting at row 1989\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.dropna()\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"label_num\"]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1710951923797,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "PrXY4fCBwsJr",
    "outputId": "c3fe1255-9e76-4689-d7a5-ac0ab396cf83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       subject: enron methanol ; meter # : 988291\\r\\n...\n",
       "1       subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
       "2       subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
       "3       subject: photoshop , windows , office . cheap ...\n",
       "4       subject: re : indian springs\\r\\nthis deal is t...\n",
       "                              ...                        \n",
       "5166    subject: put the 10 on the ft\\r\\nthe transport...\n",
       "5167    subject: 3 / 4 / 2000 and following noms\\r\\nhp...\n",
       "5168    subject: calpine daily gas nomination\\r\\n>\\r\\n...\n",
       "5169    subject: industrial worksheets for august 2000...\n",
       "5170    subject: important online banking alert\\r\\ndea...\n",
       "Name: text, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Lower\n",
    "X = X.str.lower()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1710951923797,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "Ma2t6DoZw7-g",
    "outputId": "465d9afd-95dd-463e-aea5-7f20a55e3c98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject  enron methanol   meter     988291  this is a follow up to the note i gave you on monday   4   3   00   preliminary  flow data provided by daren      please override pop   s daily volume   presently zero   to reflect daily  activity you can obtain from gas control    this change is needed asap for economics purposes  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Remove special characters\n",
    "spl_chars_removed = []\n",
    "for sentence in X:\n",
    "  spl_chars_removed.append(re.sub('[^A-Za-z0-9+]', \" \", sentence))\n",
    "X = spl_chars_removed\n",
    "\n",
    "X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 260395,
     "status": "ok",
     "timestamp": 1710952184173,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "j0oDN3Uu--mP",
    "outputId": "3c16e373-aa03-43c8-a39c-c36f09539e8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5171/5171 [04:51<00:00, 17.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. Lemmatize with Remove stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokenized_emails = []\n",
    "for text in tqdm(X):\n",
    "  doc = nlp(text)   # for each email\n",
    "  tokens = [token.lemma_ for token in doc if not (token.is_stop or token.is_space)]\n",
    "\n",
    "  tokenized_emails.append(\" \".join(tokens))\n",
    "\n",
    "X = tokenized_emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1710954731123,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "wOvxKpiqyb2N",
    "outputId": "ce554319-c0ae-4531-ac9a-8774f1a08f25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'fred', 'let\"s']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2 = spacy.blank('en')\n",
    "doc2 = nlp2('I am fred eat\\'s')\n",
    "\n",
    "vocab = [token.text for token in nlp('I am fred let\"s')]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFLt3LgUeVIl"
   },
   "source": [
    "TF-IDF vectorizer takes the vocabulary into account and calculates the Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5494,
     "status": "ok",
     "timestamp": 1710952189662,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "BcI1L729UpJT",
    "outputId": "86c47832-8f49-4eb3-9ac5-d68f50c924e1"
   },
   "outputs": [],
   "source": [
    "# 5. Vectorize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val , y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X).toarray()    # vectorizer.transform(x) can be used for already fitted Vectorizer\n",
    "\n",
    "#X_tfidf_array = X_tfidf.toarray()\n",
    "y = np.array(y)\n",
    "\n",
    "X_train_vectorized, X_test_vectorized, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.1, random_state=42)\n",
    "X_train_vectorized, X_val_vectorized , y_train, y_val = train_test_split(X_train_vectorized, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1710952189662,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "ZD-P71jhW9u8",
    "outputId": "3e4cc17d-6ba6-4c58-f112-c6ada601a3c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4187, 45992), (4187,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.shape, y_train.shape         # (5171, 46161)  - where 46161 is the size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51525,
     "status": "ok",
     "timestamp": 1710952241171,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "KtoS_JqEBGyI",
    "outputId": "a266a983-1a8d-43f4-b783-a63f50e0012d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "131/131 [==============================] - 6s 39ms/step - loss: 0.4578 - accuracy: 0.7738 - val_loss: 0.3080 - val_accuracy: 0.8970\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.1960 - accuracy: 0.9764 - val_loss: 0.1647 - val_accuracy: 0.9785\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.0948 - accuracy: 0.9947 - val_loss: 0.1088 - val_accuracy: 0.9871\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.0536 - accuracy: 0.9983 - val_loss: 0.0827 - val_accuracy: 0.9893\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 3s 23ms/step - loss: 0.0339 - accuracy: 0.9988 - val_loss: 0.0681 - val_accuracy: 0.9893\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.0230 - accuracy: 0.9993 - val_loss: 0.0600 - val_accuracy: 0.9893\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.0165 - accuracy: 0.9998 - val_loss: 0.0543 - val_accuracy: 0.9893\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.0123 - accuracy: 0.9998 - val_loss: 0.0503 - val_accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 3s 20ms/step - loss: 0.0094 - accuracy: 0.9998 - val_loss: 0.0474 - val_accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 3s 20ms/step - loss: 0.0074 - accuracy: 0.9998 - val_loss: 0.0453 - val_accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(45992,)),\n",
    "    tf.keras.layers.Dense(16),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(X_train_vectorized, y_train, validation_data=(X_val_vectorized, y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1141,
     "status": "ok",
     "timestamp": 1710955070909,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "atIq867PvY2l",
    "outputId": "437a729b-5047-49d0-bbb8-99680ccdc319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9865\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mZt1D42xiMA"
   },
   "source": [
    "### Model prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1710955117269,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "0ehFhL5JuUIs",
    "outputId": "194dba81-f9d4-46b8-e5e6-41ee91604a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ham', 'spam', 'spam', 'ham', 'ham']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_predictions(model, X_predict):\n",
    "  y_preds = model.predict(X_predict).reshape((X_predict.shape[0], ))  # the rows - number of emails\n",
    "  preds_spam_ham = ['spam' if prob==1 else 'ham' for prob in np.round(y_preds)]\n",
    "\n",
    "  return preds_spam_ham\n",
    "\n",
    "\n",
    "test_emails = vectorizer.transform(\n",
    "  ['Dinner tonight in my house', 'free money lottery coupons now', 'youve won the lottery click the link', 'Breakfast today in John\\'s house', 'The principal asked us to leave']\n",
    ").toarray()\n",
    "\n",
    "model_predictions(model, test_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1710953453684,
     "user": {
      "displayName": "Fredrick Paul A",
      "userId": "15424905174411628688"
     },
     "user_tz": -330
    },
    "id": "MqaJpju-vY_s",
    "outputId": "41aeb906-1692-4ffc-91ba-9e835bbdd5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "['ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham']\n",
      "subject hpl nom march 30 2001 attached file hplno 330 xls hplno 330 xls : ham\n",
      "subject online pharxmacy 80 med disscount phafrmacy onlsine grasnd opegne 80 med orfder today doorst : spam\n",
      "subject nom actual volume april 17 th agree eileen ponton 04 18 2001 10 48 19 david avila lsp enserc : ham\n",
      "subject meter 8740 dec 99 robert head decide enter partnership agreement day question party thu real : ham\n",
      "subject coastal oil gas corporation melissa deal 348450 create enter sitara addition volume edit 3 0 : ham\n",
      "subject enron hpl actual july 27 2000 teco tap 25 000 enron 48 750 hpl iferc ls hpl lsk ic 15 000 en : ham\n",
      "subject meter 98 9699 98 2662 receipt meter w delivery nom fuel buy sell hplc hplr meter 9699 s 2 5  : ham\n",
      "subject hpl nom 3 2001 attached file hplno 503 xls hplno 503 xls : ham\n",
      "subject enron hpl nom february 23 25 2001 february 23 2001 teco tap 30 000 february 24 2001 teco tap : ham\n",
      "subject tenaska iv transport 10 00 take look transport txu lone star tenaska iv 10 00 price differen : ham\n"
     ]
    }
   ],
   "source": [
    "predictions = model_predictions(model, X_test_vectorized[0:10])\n",
    "print(predictions)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    print(X_test[i][:100], ':', predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxnKiw+7aFFxa87FyuycfM",
   "mount_file_id": "1g4mdcU9TD0VyH630Ofo-5qVTYJB80ade",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
